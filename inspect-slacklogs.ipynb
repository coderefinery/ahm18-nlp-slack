{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Slack logs from NeIC channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- use examples from github.com/jalajthanaki/NLPython\n",
    "- train sentiment analysis engine on some samples\n",
    "- remove backslashes from words like it's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "print(sys.version)\n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding('utf8')\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir slack_logs\n",
    "cd slack_logs\n",
    "mv ../NeIC_Slack_export_Dec10_2017.zip .\n",
    "unzip NeIC_Slack_export_Dec10_2017.zip\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls slack_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = %sx ls -d slack_logs/*/\n",
    "for n,i in enumerate(dirs):\n",
    "    print(n,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first try out one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = dirs[3] #ahm-planning\n",
    "dir = dirs[13] #coderefinery\n",
    "\n",
    "os.listdir(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the structure of the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = os.listdir(dir)\n",
    "d = dates[0] \n",
    "with open(dir+d,\"r\") as f:\n",
    "    raw_json = json.loads(f.read())\n",
    "dump = json.dumps(raw_json,indent=4)\n",
    "print(dump)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, `subtype` only present if it's not a regular message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in raw_json:\n",
    "    if \"subtype\" in j.keys():\n",
    "        print (j[\"type\"], j[\"subtype\"], j[\"text\"])\n",
    "    else:\n",
    "        print(j[\"type\"], j[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract all regular messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dates = os.listdir(dir)\n",
    "messages = []\n",
    "for d in dates: \n",
    "    with open(dir+d,\"r\") as f:\n",
    "        raw_json = json.loads(f.read())\n",
    "\n",
    "    for j in raw_json:\n",
    "        if not \"subtype\" in j.keys(): # exclude non-message messages\n",
    "            messages.append(j[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add all words in all messages to one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "words = []\n",
    "for m in messages:\n",
    "    for w in m.split(): # split up words in messages\n",
    "        #w = re.sub('\\s+', '', w) # remove any whitespace character [ \\t\\n\\r\\f\\v]\n",
    "        # keep delimiters ,.!? by using regex group, and keep multiple symbols together\n",
    "        w = re.split(r'([,.!?]+)', w)\n",
    "        for ww in w: # split up delimiters\n",
    "            if len(ww)>0: # get rid of empty strings\n",
    "                words.append(ww)\n",
    "\n",
    "    # add fullstop to end of messages if needed\n",
    "    if not (re.match(r\"[.,!?]\", m[-1])):\n",
    "        words.append(\".\")\n",
    "    \n",
    "words[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's do this for all the Slack channels:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_channels = [d.replace(\"slack_logs/\",\"\").replace(\"/\",\"\") for d in dirs]\n",
    "# dictionary which will hold all words from each channel\n",
    "words_in_channels = dict.fromkeys(all_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to join messages into one long array\n",
    "import re\n",
    "def join_messages(messages):\n",
    "    words = []\n",
    "    for m in messages:\n",
    "        for w in m.split(): # split up words in messages\n",
    "            # not needed with python split() method...\n",
    "            #w = re.sub('\\s+', '', w) # remove any whitespace character [ \\t\\n\\r\\f\\v]\n",
    "            # keep delimiters ,.!? by using re group, and keep multiple symbols together\n",
    "\n",
    "            # try ignoring delimiters instead:\n",
    "#            w = re.split(r'([,.!?]+)', w)\n",
    "#            for ww in w: # split up delimiters\n",
    "#                if len(ww)>0: # get rid of empty strings\n",
    "#                    words.append(ww.lower()) # make all strings lowercase\n",
    "            words.append(w.lower())\n",
    "\n",
    "        # add fullstop to end of messages if needed\n",
    "        if len(m)>0: # need to exclude empty messages\n",
    "            if not (re.match(r\"[.,!?]\", m[-1])):\n",
    "                words.append(\".\")\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join messages in all channels into elements of words_in_channels dict\n",
    "for channel in all_channels:\n",
    "    #print(\"channel %s\"%channel)\n",
    "    dates = os.listdir(\"slack_logs/\"+channel)\n",
    "    messages = []\n",
    "    for d in dates: \n",
    "        with open(\"slack_logs/\"+channel+\"/\"+d,\"r\") as f:\n",
    "            raw_json = json.loads(f.read())\n",
    "\n",
    "        for j in raw_json:\n",
    "            if not \"subtype\" in j.keys(): # exclude non-message messages\n",
    "                messages.append(j[\"text\"])\n",
    "    words_in_channels[channel] = join_messages(messages)\n",
    "    print(\"channel {} has {} words\".format(channel,len(words_in_channels[channel])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove empty channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in words_in_channels.keys():\n",
    "    if len(words_in_channels[i])==0:\n",
    "        words_in_channels.pop(i, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "x = words_in_channels.keys()\n",
    "y = [len(words_in_channels[i]) for i in words_in_channels.keys()]\n",
    "ax = sns.barplot(x=y, y=x)\n",
    "\n",
    "ax.set_xlim([0,200000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, let's focus on the largest channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [\"tryggve\",\"general\",\"xt\",\"web\",\"random\",\"arc-debugging\",\"ndgf\",\"coderefinery\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language toolkit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > Note: lots of extra packages need to be downloaded with `nltk.download()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(set(words))\n",
    "most_common_words = dict.fromkeys(channels,0)\n",
    "dists = dict.fromkeys(channels,0)\n",
    "for channel in channels:\n",
    "    words = words_in_channels[channel]\n",
    "    dist = nltk.FreqDist(words)\n",
    "    dists[channel] = dist\n",
    "    most_common_words[channel] = dist.most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(most_common_words[\"coderefinery\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(data=most_common)\n",
    "df_words.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = []\n",
    "for index, row in df_words.iterrows():\n",
    "    for r in row:\n",
    "        common_words.append(r[0])\n",
    "common_words = set(common_words)\n",
    "common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Exercise: create a new dataframe with the common words as indices and number of appearances as values alon rows*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at lexical diversity, i.e. ratio of number of distinct words and total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels: # loop over the largest channels\n",
    "    words = words_in_channels[channel]\n",
    "    lex_div = lexical_diversity(words)\n",
    "    print(\"Lexical diversity in %s is %f\"%(channel,lex_div))\n",
    "#len(set(words))/len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Sentiment analysis\": emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emojis in the Slack logs are expressed like `:slightly_smiling_face:` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_emojis = dict.fromkeys(channels,0)\n",
    "dists_emojis = dict.fromkeys(channels,0)\n",
    "\n",
    "for channel in channels:\n",
    "    words = words_in_channels[channel]\n",
    "    # need to join words for the FreqDist method\n",
    "    all_words = \" \".join(words)\n",
    "\n",
    "    #emojis = re.findall(r\":\\w*:\",all_words) # this includes strings like :43:\n",
    "    emojis = re.findall(r\":[a-zA-Z_]+:\",all_words) # this filters out strings like :43:\n",
    " \n",
    "    dist = nltk.FreqDist(emojis)\n",
    "    dist.most_common(20)\n",
    "    \n",
    "    dists_emojis[channel] = dist\n",
    "    most_common_emojis[channel] = dist.most_common(40)\n",
    "    \n",
    "most_common_emojis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the unique emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_emojis = set()\n",
    "for i in most_common_emojis.keys():\n",
    "    for j in most_common_emojis[i]:\n",
    "        unique_emojis.add(j[0])\n",
    "unique_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate a few key emojis, in increasing order of positivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "key_emojis = [u\":disappointed:\",u\":confused:\",u\":flushed:\",u\":wink:\",u\":slightly_smiling_face:\",\n",
    "              u\":thumbsup:\",u\":clap:\",u\":stuck_out_tongue:\",u\":smile:\",  \n",
    "               u\":grinning:\",u\":heart_eyes:\",u\":laughing:\"]\n",
    "#print \"hello\"\n",
    "for i in key_emojis:\n",
    "    print i\n",
    "    print(emoji.emojize('NeIC is %s'%i, use_aliases=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = most_common_emojis.copy()\n",
    "for i in most_common_emojis:\n",
    "#for i in tmp:\n",
    "    excluded_emojis = [x[0] for x in most_common_emojis[i] if x[0] in key_emojis]\n",
    "#    excluded_emojis = [x[0] for x in tmp[i] if x[0] in key_emojis]\n",
    "    l1 = [x for x in most_common_emojis[i] if x[0] in key_emojis]\n",
    "    l2 = [(x,0) for x in key_emojis if x not in excluded_emojis]\n",
    "    print l1\n",
    "    print l2\n",
    "    newlist = l1 + l2\n",
    "    newlist.sort()\n",
    "    most_common_emojis[i] = newlist\n",
    "    #tmp[i] = newlist.sort()\n",
    "most_common_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emojis = pd.DataFrame(data=most_common_emojis)\n",
    "df_emojis.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the emojis from one of the columns, and set it to the row indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis, y = zip(*df_emojis.coderefinery)\n",
    "df_emojis['emojis'] = emojis\n",
    "df_emojis.set_index('emojis',drop=True, inplace=True)\n",
    "df_emojis.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do better, let's create real emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis2=[]\n",
    "for i in emojis:\n",
    "    x = emoji.emojize(i, use_aliases=True) \n",
    "    emojis2.append(x)\n",
    "df_emojis[\"emojis\"] = emojis2\n",
    "df_emojis.set_index('emojis',inplace=True)\n",
    "df_emojis.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract only the numbers from the dataframe tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_emojis:\n",
    "    dummy, y = zip(*df_emojis[x])\n",
    "    df_emojis[x] = y\n",
    "df_emojis.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to total number of selected emojis in each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = 100*df_emojis/df_emojis.sum()\n",
    "df_tmp.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "(virtually speaking...)\n",
    "- NeIC people smile a lot overall \n",
    "- There's not a lot of clapping and thumbs-up-giving, except for `XT` people who gesticulate quite a lot with their hands\n",
    "- On the other hand, `XT`-ers don't slightly as much as other channels, but they laugh quite a bit\n",
    "- The most ambiguous communication takes place on `random` and `general`, as evidenced by the high proportion of winking\n",
    "- `NDGF`-ers are the most disappointed channel. Anything we can do to help guys? 😉 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't really need a heatmap plot\n",
    "\n",
    "#emojis currently don't work as y-labels\n",
    "#%matplotlib inline\n",
    "##normalize:\n",
    "#plt.rcParams[\"figure.figsize\"] = [12.0, 8.0]\n",
    "#plt.rcParams['figure.dpi'] = 300\n",
    "#sns.set(font='Segoe UI Emoji')\n",
    "#norm = 100*df_emojis / df_emojis.sum()\n",
    "\n",
    "#g = sns.heatmap(norm,linewidths=.5,annot=True,cbar=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collocations and bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('treebank')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    words = words_in_channels[channel]\n",
    "    all_words = \" \".join(words)\n",
    "    tokens = nltk.word_tokenize(all_words)\n",
    "    text = nltk.Text(tokens)\n",
    "    print(channel)\n",
    "    print(\"------------\")\n",
    "    text.collocations()\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Searching for words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do people in different channels find fantastic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    words = words_in_channels[channel]\n",
    "    all_words = \" \".join(words)\n",
    "    tokens = nltk.word_tokenize(all_words)\n",
    "    text = nltk.Text(tokens)\n",
    "    print(channel)\n",
    "    print(\"------------\")\n",
    "    text.concordance(\"fantastic\")\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that appear in similar contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    words = words_in_channels[channel]\n",
    "    all_words = \" \".join(words)\n",
    "    tokens = nltk.word_tokenize(all_words)\n",
    "    text = nltk.Text(tokens)\n",
    "    print(channel)\n",
    "    print(\"------------\")\n",
    "    text.similar(\"good\")\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First need some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll join all words for each channel\n",
    "joined_words_in_channels = dict.fromkeys(channels)\n",
    "for i in channels:\n",
    "    words = words_in_channels[i]\n",
    "    joined_words_in_channels[i] = \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textmining\n",
    "\n",
    "tdm = textmining.TermDocumentMatrix()\n",
    "for channel in channels:\n",
    "    tdm.add_doc(joined_words_in_channels[channel])\n",
    "\n",
    "# write term document matrix to csv file\n",
    "tdm.write_csv('matrix2.csv', cutoff=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for row in tdm.rows(cutoff=1):\n",
    "#    print type(row)\n",
    "vocab = list(tdm.rows(cutoff=1))[0]\n",
    "titles = channels\n",
    "X = np.array(list(tdm.rows(cutoff=1))[1:])\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(X)  # model.fit_transform(X) is also available\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the fit model we can look at the topic-word probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = model.topic_word_\n",
    "\n",
    "for n in range(5):\n",
    "    sum_pr = sum(topic_word[n,:])\n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]\n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other information we get from the model is document-topic probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "for n in range(8):\n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    print(\"doc: {} topic: {}\\n{}...\".format(n,topic_most_pr,titles[n][:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax= plt.subplots(8, 1, figsize=(8, 12), sharex=True)\n",
    "#for i, k in enumerate([0, 1, 2, 3, 4, 5, 6, 7]):\n",
    "for i, k in enumerate([0, 1, 2, 3,4,5,6,7]):\n",
    "    ax[i].stem(doc_topic[k,:], linefmt='r-',\n",
    "               markerfmt='ro', basefmt='w-')\n",
    "    ax[i].set_xlim(-0.5, 19.5)\n",
    "    ax[i].set_xticks(range(20))\n",
    "    ax[i].set_ylim(0, .5)\n",
    "    ax[i].set_ylabel(\"Prob\")\n",
    "    ax[i].set_title(\"{}\".format(channels[k]))\n",
    "\n",
    "ax[7].set_xlabel(\"Topic\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f, ax= plt.subplots(5, 1, figsize=(8, 6), sharex=True)\n",
    "#for i, k in enumerate([0, 5, 9, 14, 19]):\n",
    "#    ax[i].stem(topic_word[k,:], linefmt='b-',\n",
    "#               markerfmt='bo', basefmt='w-')\n",
    "#    ax[i].set_xlim(-50,4350)\n",
    "#    ax[i].set_ylim(0, 0.08)\n",
    "#    ax[i].set_ylabel(\"Prob\")\n",
    "#    ax[i].set_title(\"topic {}\".format(k))\n",
    "\n",
    "#ax[4].set_xlabel(\"word\")\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AHM2018)",
   "language": "python",
   "name": "ahm2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
