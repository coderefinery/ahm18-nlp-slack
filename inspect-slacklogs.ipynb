{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing of NeIC Slack logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First fetch the source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First clone with  \n",
    "`git clone https://github.com/coderefinery/ahm18-nlp-slack.git`  \n",
    "or  \n",
    "`git clone git@github.com:coderefinery/ahm18-nlp-slack.git`  \n",
    "(or fork first and then clone!)\n",
    "\n",
    "and then  \n",
    "```cd ahm18-nlp-slack\n",
    "jupyter-notebook```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need the Slack logs! These can be viewed at [https://wiki.neic.no/chat/](https://wiki.neic.no/chat/), but webscraping it is difficult.\n",
    "\n",
    "Instead, go to this [Google Drive link](https://drive.google.com/open?id=1BVioZ9t15c1Ek7Xq067stN0ED28eXOsJ) and save the zipfile to the current directory (thanks to Joel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some background on Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History of Jupyter\n",
    "  - In 1991, Guido van Rossum publishes Python, which starts to gain in popularity\n",
    "  - In 2001, Fernando PÃ©rez started programming a fancier shell for Python called IPython\n",
    "  - In 2014, Fernando PÃ©rez announced a spin-off project from IPython called Project Jupyter. IPython will continue to exist as a Python shell and a kernel for Jupyter, while the notebook and other language-agnostic parts of IPython will move under the Jupyter name\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why \"Jupyter\"?\n",
    " - Julia + Python + R\t      \n",
    " - Jupyter is actually language agnostic and Jupyter kernels exist for dozens of programming languages\n",
    " - Galileo's publication in a pamphlet in 1610 in Sidereus Nuncius about observations of Jupiter's moons is formulated as a notebook, with illustrations, text, calculations, titles, datapoints, images, reasoning... One of the first notebooks!  \n",
    "<img src=\"http://media.gettyimages.com/photos/pages-from-sidereus-nuncius-magna-by-galileo-galilei-a-book-of-and-picture-id90732970\" width=\"500\">\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases\n",
    "- Experimenting with new ideas, testing new libraries/databases \n",
    "- Interactive code and visualization development\n",
    "- Sharing and explaining code to colleagues\n",
    "- Learning from other notebooks\n",
    "- Interactive data analysis\n",
    "- Many cloud platforms offer access to Jupyter Notebooks \n",
    "- Keeping track of interactive sessions, like a digital lab notebook\n",
    "- Supplementary information with published articles\n",
    "- Teaching (programming, experimental/theoretical science)\n",
    "- Presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cells\n",
    "\n",
    "- **Code cells** contain code to be interpreted by the *kernel* (Python, R, Julia, Octave/Matlab...)\n",
    "- **Markdown cells** contain formatted text written in Markdown \n",
    "![Components](img/notebook_components.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown cells\n",
    "\n",
    "This cell contains simple [markdown](https://daringfireball.net/projects/markdown/syntax), a simple language for writing text that can be automatically converted to other formats, e.g. HTML, LaTeX or any of a number of others.\n",
    "\n",
    "**Bold**, *italics*, **_combined_**, ~~strikethrough~~, `inline code`.\n",
    "\n",
    "* bullet points\n",
    "\n",
    "or\n",
    "\n",
    "1. numbered\n",
    "3. lists\n",
    "\n",
    "**Equations:**   \n",
    "inline $e^{i\\pi} + 1 = 0$\n",
    "or on new line  \n",
    "$$e^{i\\pi} + 1 = 0$$\n",
    "\n",
    "Images ![CodeRefinery Logo](https://pbs.twimg.com/profile_images/875283559052980224/tQLhMsZC_400x400.jpg)\n",
    "\n",
    "Links:  \n",
    "[One of many markdown cheat-sheets](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#emphasis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a code cell can run statements of code.\n",
    "# when you run this cell, the output is sent \n",
    "# from the web page to a back-end process, run \n",
    "# and the results are displayed to you\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of execution is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful keyboard shortcuts \n",
    "\n",
    "Some shortcuts only work in Command or Edit mode.\n",
    "\n",
    "* `Enter` key to enter Edit mode (`Escape` to enter Command mode)\n",
    "* `Ctrl`-`Enter`: run the cell\n",
    "* `Shift`-`Enter`: run the cell and select the cell below\n",
    "* `Alt`-`Enter`: run the cell and insert a new cell below\n",
    "* `Ctrl`-`s`: save the notebook \n",
    "* `Tab` key for code completion or indentation (Edit mode)\n",
    "* `m` and `y` to toggle between Markdown and Code cells (Command mode)\n",
    "* `d-d` to delete a cell (Command mode)\n",
    "* `z` to undo deleting (Command mode)\n",
    "* `a/b` to insert cells above/below current cell (Command mode)\n",
    "* `x/c/v` to cut/copy/paste cells (Command mode)\n",
    "* `Up/Down` or `k/j` to select previous/next cells (Command mode)\n",
    "* `h` for help menu for keyboard shortcuts (Command mode)\n",
    "* Append `?` for help on commands/methods, `??` to show source (Edit mode) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**: Spend a few minutes playing around in the notebook. Add cells, toggle between Markdown and Code, execute some code, write some markdown. Try the keyboard shortcuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links and further reading\n",
    " - http://nbviewer.jupyter.org/\n",
    " - https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks\n",
    " - http://mybinder.org/\n",
    " - https://jupyterhub.readthedocs.io/en/latest/\n",
    " - http://ipython-books.github.io/minibook/\n",
    " - http://ipython-books.github.io/cookbook/\n",
    " - https://www.oreilly.com/ideas/the-state-of-jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Slack logs\n",
    "\n",
    "Let's get down to business.  \n",
    "Hopefully everyone has the following packages installed:\n",
    "- `numpy`, `pandas`, `matplotlib`, `jupyter`, `nltk`, `textmining`, `lda`, `emoji`, `json`, `re`, `datetime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "print(sys.version)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import emoji\n",
    "import lda\n",
    "import textmining\n",
    "import datetime\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have a code cell, but it's not Python! Have a look at the first line. `%%bash` is a *cell magic* which tells Jupyter to interpret the contents of the cell as bash commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir slack_logs\n",
    "cd slack_logs\n",
    "mv ../NeIC_Slack_export_Dec10_2017.zip .\n",
    "unzip NeIC_Slack_export_Dec10_2017.zip\n",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect directory structure and file format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below we have a *line magic* `%sx` (with one `%` sign) which runs a shell command (using `commands.getoutput()`) and captures the output.  \n",
    "\n",
    "If you want to see what magic commands are available, type `%lsmagic` in a code cell. Magics depend on what kernel is used, and new magics can also be installed (and created!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = %sx ls -d slack_logs/*/\n",
    "for n,i in enumerate(dirs):\n",
    "    print(n,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Add a code cell(s) below. Use either a bash cell magic or sx line magics to list the contents of the `_aa` directory under `slack_logs`, `cat` the contents of the json file, and then remove the `_aa` directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first explore one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note how a scrollable window appears in the output area of this cell\n",
    "dir = dirs[13] #coderefinery\n",
    "os.listdir(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the structure of the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = os.listdir(dir)\n",
    "d = dates[0] # pick the first date\n",
    "\n",
    "# read in contents of json file\n",
    "with open(dir+d,\"r\") as f:\n",
    "    raw_json = json.loads(f.read())\n",
    "\n",
    "# dump json\n",
    "dump = json.dumps(raw_json,indent=4)\n",
    "print(dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, `subtype` only present if it's not a regular message. We also see that each message has a unix epoch timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract all regular messages in one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dates = os.listdir(dir) # this is still the coderefinery channel\n",
    "messages = []\n",
    "\n",
    "for d in dates: \n",
    "    with open(dir+d,\"r\") as f:\n",
    "        raw_json = json.loads(f.read())\n",
    "\n",
    "    for j in raw_json:\n",
    "        if not \"subtype\" in j.keys(): # exclude non-message messages\n",
    "            messages.append(j[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add all words in all messages to one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for m in messages:\n",
    "    [words.append(w.lower()) for w in m.split()]\n",
    "    \n",
    "words[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do this for all the Slack channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with all channel names\n",
    "all_channels = [d.replace(\"slack_logs/\",\"\").replace(\"/\",\"\") for d in dirs]\n",
    "\n",
    "# dictionary with channel names as keys\n",
    "words_in_channels = dict.fromkeys(all_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to join messages into one long array\n",
    "def join_messages(messages):\n",
    "    words = []\n",
    "    for m in messages:\n",
    "        [words.append(w.lower()) for w in m.split()]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now join all words in all channels. Is this time-consuming? Let's time it with the `%%timeit` cell magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "# join messages in all channels into elements of words_in_channels dict\n",
    "for channel in all_channels:\n",
    "    dates = os.listdir(\"slack_logs/\"+channel)\n",
    "    messages = []\n",
    "    for d in dates: \n",
    "        with open(\"slack_logs/\"+channel+\"/\"+d,\"r\") as f:\n",
    "            raw_json = json.loads(f.read())\n",
    "\n",
    "        for j in raw_json:\n",
    "            if not \"subtype\" in j.keys(): # exclude non-message messages\n",
    "                messages.append(j[\"text\"])\n",
    "    words_in_channels[channel] = join_messages(messages)\n",
    "    print(\"channel {} has {} words\".format(channel,len(words_in_channels[channel])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove empty channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_channels = { k:v for k,v in words_in_channels.items() if len(v)!=0 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot number of words in channels using Seaborn barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12,9]\n",
    "x = words_in_channels.keys()\n",
    "y = [len(words_in_channels[i]) for i in words_in_channels.keys()]\n",
    "ax = sns.barplot(x=y, y=x);\n",
    "\n",
    "ax.set_xlim([0,200000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, let's focus on the largest channels (and include `ahm-planning` for good measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [\"tryggve\",\"general\",\"xt\",\"web\",\"random\",\"arc-debugging\",\"ndgf\",\"coderefinery\", \"ahm-planning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple natural language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language toolkit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tab completion can be used to see available methods of a module\n",
    "#nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can view docstrings for functions in python modules: \n",
    "nltk.FreqDist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the source code itself can be viewed by using two question marks!\n",
    "nltk.FreqDist??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find frequency distribution of words, but excluding stopwords!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "most_common_words = dict.fromkeys(channels,0)\n",
    "dists = dict.fromkeys(channels,0)\n",
    "stop = stopwords.words('english')\n",
    "for channel in channels:\n",
    "   words = words_in_channels[channel]\n",
    "   words = [token for token in words if token not in stop]\n",
    "   dist = nltk.FreqDist(words)\n",
    "   dists[channel] = dist\n",
    "   most_common_words[channel] = dist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas dataframes\n",
    "\n",
    "We create a dataframe to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can feed a dictionary to the `DataFrame` method\n",
    "df_words = pd.DataFrame(data=most_common_words)\n",
    "df_words.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get a list of unique common words with a set()\n",
    "common_words = set()\n",
    "for index, row in df_words.iterrows():\n",
    "    for r in row:\n",
    "        common_words.add(r[0])\n",
    "for i in common_words:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical diversity (type-token ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at lexical diversity, i.e. the ratio of number of distinct words and total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels: # loop over the largest channels\n",
    "    words = words_in_channels[channel]\n",
    "    lex_div = lexical_diversity(words)\n",
    "    print(\"Lexical diversity in %s is %f\"%(channel,lex_div))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linguistic richness is clearly greatest in `ahm-planning`, closely followed by `random` and `general`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations, contexts and similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.Text.collocations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collocations (sequences of words that co-occur more often than expected by chance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    words = words_in_channels[channel]\n",
    "    all_words = \" \".join(words)\n",
    "    tokens = nltk.word_tokenize(all_words)\n",
    "    text = nltk.Text(tokens)\n",
    "    print(channel)\n",
    "    print(\"------------\")\n",
    "    text.collocations()\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tyttebÃ¤r hangout??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similar contexts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.Text.similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    words = words_in_channels[channel]\n",
    "    all_words = \" \".join(words)\n",
    "    tokens = nltk.word_tokenize(all_words)\n",
    "    text = nltk.Text(tokens)\n",
    "    print(channel)\n",
    "    print(\"------------\")\n",
    "    text.similar(\"good\")\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Searching for words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Another method of `nltk.Text` is `concordance`, which finds all matches in a body of text for a given word or phrase.  \n",
    "1. Have a look at the docstring for `nltk.Text.concordance` to see how it's used.\n",
    "2. Copy-paste either the `collocation` or `similar` cell from above using keyboard shortcuts `C` and `V`.\n",
    "3. Edit the cell to use the `concordance` method, with a word or phrase of your choice \n",
    "4. Share if you find something interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Sentiment analysis\": emojis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sentiment analysis (a.k.a. opinion mining or emotion AI)](https://en.wikipedia.org/wiki/Sentiment_analysis) refers to the use of NLP, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.   \n",
    "Used on reviews, survey responses, online and social media, healthcare materials..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will abuse this concept and analyze sentiments in NeIC Slack channels through emoji usage.  \n",
    "Emojis in Slack logs are expressed like `:slightly_smiling_face:` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find all the emojis first by joining all words from all channels and regex-ing it, and then find the most frequent ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join words from all channels into one list, and then join into one long \"sentence\"\n",
    "words = [j for i in words_in_channels.values() for j in i] \n",
    "all_words = \" \".join(words)\n",
    "\n",
    "emojis = re.findall(r\":[a-zA-Z_]+:\",all_words) # this filters out strings like :43:\n",
    "\n",
    "dist = nltk.FreqDist(emojis)\n",
    "dist.most_common(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate a few key emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_emojis = [u\":disappointed:\",u\":wink:\",u\":slightly_smiling_face:\",u\":simple_smile:\",\n",
    "              u\":thumbsup:\",u\":clap:\",u\":stuck_out_tongue:\",u\":smile:\",u\":grinning:\"]\n",
    "\n",
    "for i in common_emojis:\n",
    "    print(emoji.emojize('NeIC is %s'%i, use_aliases=True)) \n",
    "    print(i)\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm, `emoji` package doesn't understand `:simple_smile:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how often emojis are used in the different channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_emojis = dict.fromkeys(channels,0)\n",
    " \n",
    "for channel in channels:\n",
    "    words = words_in_channels[channel]\n",
    "    count = [words.count(i) for i in common_emojis]\n",
    "    channel_emojis[channel] = count\n",
    "\n",
    "channel_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the dict into a DataFrame for easy manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emojis = pd.DataFrame(data=channel_emojis)\n",
    "df_emojis.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use real emojis as indices in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis=[]\n",
    "for i in common_emojis:\n",
    "    x = emoji.emojize(i, use_aliases=True) \n",
    "    emojis.append(x)\n",
    "df_emojis[\"emojis\"] = emojis\n",
    "df_emojis.set_index('emojis',inplace=True, drop=True)\n",
    "df_emojis.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look good with the unsupported emoji. Let's sum simple_smile into slightly_smiling_face (we in the Nordics don't have such fine-grained positive emotions anyways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_keep = df_emojis.index[2]\n",
    "row_delete = df_emojis.index[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add row_delete to row_keep, and delete row_delete\n",
    "df_emojis.loc[row_keep] += df_emojis.loc[row_delete]\n",
    "df_emojis.drop([row_delete], inplace=True)\n",
    "\n",
    "df_emojis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to total number of selected emojis in each channel to get final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = 100*df_emojis/df_emojis.sum()\n",
    "df_tmp.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "- NeIC people express quite positive emotions overall \n",
    "- There's not a lot of clapping and thumbs-up-giving, except for `XT` people who do both, and `Tryggve` people enjoy clapping \n",
    "- On the other hand, `XT`-ers don't smile as much as other channels, but they do laugh a bit\n",
    "- People in `web` stick out their tongue more than average\n",
    "- `arc-debugging` and `ahm-planning` folks are rather limited in their emotional repertoire  \n",
    "- The most ambiguous communication takes place on `random` and `general`, as evidenced by the high proportion of winking\n",
    "- `NDGF`-ers are the most disappointed channel. Anything we can do to help guys? ðŸ˜‰ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling with latent Dirichlet allocation (LDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic models are statistical models for discovering topics that occur in a set of documents. For a visual representation of what goes on in topic modelling, [see this animation](https://upload.wikimedia.org/wikipedia/commons/7/70/Topic_model_scheme.webm).  \n",
    "\n",
    "[LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is probably the most commonly used topic model today and was introduced by [David Blei, Andrew Ng and Michael I. Jordan](http://www.jmlr.org/papers/v3/blei03a.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some preprocessing. First join all words for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_words_in_channels = dict.fromkeys(channels)\n",
    "for i in channels:\n",
    "    words = words_in_channels[i]\n",
    "    joined_words_in_channels[i] = \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`textmining` is a useful package to create the [term-document matrix](https://en.wikipedia.org/wiki/Document-term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = textmining.TermDocumentMatrix()\n",
    "# add documents to the term-document matrix (each channel is a document)\n",
    "for channel in channels:\n",
    "    tdm.add_doc(joined_words_in_channels[channel])\n",
    "\n",
    "# write term document matrix to csv file\n",
    "tdm.write_csv('matrix.csv', cutoff=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a numpy array as input to the LDA fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(tdm.rows(cutoff=1))[0] #needed later\n",
    "titles = channels\n",
    "X = np.array(list(tdm.rows(cutoff=1))[1:])\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the model, and run the LDA fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "model = lda.LDA(n_topics=n_topics, n_iter=1500, random_state=1)\n",
    "model.fit(X)  # model.fit_transform(X) is also available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the fit model we can look at the topic-words. Let's look at the top 20 words for each topic by probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = model.topic_word_\n",
    "topic_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 20\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other information we get from the model is document-topic probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "doc_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(channels)):\n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    print(\"doc: {} topic: {}\\n{}...\".format(n,topic_most_pr,titles[n][:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that wasn't interesting. The top topics are just help words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the topics instead, and exclude topics 0 and 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax= plt.subplots(len(channels), 1, figsize=(8, 12))\n",
    "for k in range(len(channels)):\n",
    "    ax[k].stem(doc_topic[k,:], linefmt='r-',\n",
    "               markerfmt='ro', basefmt='w-')\n",
    "    ax[k].set_xlim(0.5, 18.5)\n",
    "    ax[k].set_xticks(range(1,19))\n",
    "    ax[k].set_ylim(0, .3)\n",
    "    ax[k].set_ylabel(\"Prob\")\n",
    "    ax[k].set_title(\"{}\".format(channels[k]))\n",
    "\n",
    "ax[-1].set_xlabel(\"Topic\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, let's also make a HTML table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=[[] for i in range(len(topic_word))]\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    table[i].append(str(i))\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    table[i].append(' '.join(topic_words))\n",
    "from IPython.display import HTML, display\n",
    "display(HTML('<table><tr>{}</tr></table>'.format('</tr><tr>'.join(\n",
    "            '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in table))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "- Clearly, different topics are being discussed in each channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Redo the above LDA analysis *removing stop words first*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: time analysis\n",
    "\n",
    "#### When are messages are posted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unixtime_in_channels = dict.fromkeys(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join timestamps in all channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in channels:\n",
    "    dates = os.listdir(\"slack_logs/\"+channel)\n",
    "    unixtime = []\n",
    "    for d in dates: \n",
    "        with open(\"slack_logs/\"+channel+\"/\"+d,\"r\") as f:\n",
    "            raw_json = json.loads(f.read())\n",
    "\n",
    "        for j in raw_json:\n",
    "            if not \"subtype\" in j.keys(): # exclude non-message messages\n",
    "                unixtime.append(j[\"ts\"])\n",
    "    unixtime_in_channels[channel] = unixtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert to readable timestamps (and use a list this time, for no particular reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timestamps = []\n",
    "for channel in channels:\n",
    "    unixtime = unixtime_in_channels[channel]\n",
    "    timestamps = [datetime.datetime.fromtimestamp(int(float(i))).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                  for i in unixtime]\n",
    "    all_timestamps.append(timestamps)\n",
    "    print(\"first message in {} was posted on {}\".format(channel,timestamps[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert to a pandas DataFrame, since they're nice to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datetimes = pd.DataFrame(data=all_timestamps)\n",
    "df_datetimes = df_datetimes.transpose()\n",
    "# set column labels to channel names\n",
    "df_datetimes.columns = channels\n",
    "df_datetimes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the groupby method to group dataframe into hours, and then count to get histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hours = pd.DataFrame()\n",
    "for channel in channels:\n",
    "    # need to convert to datetime64\n",
    "    try:\n",
    "        df_datetimes[channel] = df_datetimes[channel].astype(\"datetime64\")\n",
    "    except:\n",
    "        pass\n",
    "    # groupby and count()\n",
    "    df_hours[channel] = df_datetimes[channel].groupby(df_datetimes[channel].dt.hour).count()\n",
    "\n",
    "# normalize    \n",
    "df_hours = df_hours / df_hours.sum()\n",
    "# set name of index column\n",
    "df_hours.rename_axis(\"Hour\",inplace=True)\n",
    "df_hours.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hours.plot(kind=\"bar\",subplots=True,figsize=(10,18),ylim=(0,0.25));\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise:** Do the same analysis but splitting into weekdays!   \n",
    "> (hint: format specifier %A gives the weekday)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AHM2018)",
   "language": "python",
   "name": "ahm2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
